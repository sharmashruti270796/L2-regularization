{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    x=np.arange(1,len(dim)+1)\n",
    "    w=np.zeros_like(x)\n",
    "    b=0\n",
    "    \n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_weights(w,b):\n",
    "    assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sig = 1/(1+math.exp(-z))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_sigmoid(z):\n",
    "    val=sigmoid(z)\n",
    "    assert(val==0.8807970779778823)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    sum=0\n",
    "    for each in range(len(y_true)):\n",
    "        l = ((y_true[each]) * (math.log10(y_pred[each])) + ((1-y_true[each])* (math.log10(1-y_pred[each]))))\n",
    "        sum+=l\n",
    "    loss = (-1) * (sum/len(y_true))\n",
    "            \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_logloss(true,pred):\n",
    "    loss=logloss(true,pred)\n",
    "    assert(loss==0.07644900402910389)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    z = np.dot(w,x+ b)\n",
    "    sigma = 1/(1 + math.exp(-z))\n",
    "    \n",
    "    lam= 0.001\n",
    "    dw = x * (y - sigma) - ((lam * w)/N)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "    assert(np.sum(grad_dw)==2.613689585)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient_db(x,y,w,b):\n",
    "        #z = np.dot(w,x+ b)\n",
    "        #sigma = 1/(1 + math.exp(-z))\n",
    "        db = y - sigmoid(np.dot(w,x+b))\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "    grad_db=gradient_db(x,y,w,b)\n",
    "    assert(grad_db==-0.5)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    N=len(X_train)\n",
    "    w,b = initialize_weights(X_train[0])\n",
    "    for i in range(epochs):\n",
    "        y_train_pred=[]\n",
    "        y_test_pred=[]\n",
    "        for j in range(N):\n",
    "            dw = gradient_dw(X_train[j],y_train[j],w,b,alpha,N)\n",
    "            db = gradient_db(X_train[j],y_train[j],w,b)\n",
    "            w = w + (alpha*dw)\n",
    "            b = b + (alpha*db)\n",
    "\n",
    "        for j in range(len(X_train)):\n",
    "\n",
    "            y_train_pred.append(sigmoid(np.dot(w,X_train[j]+b)))\n",
    "        for j in range(len(X_test)):    \n",
    "            y_test_pred.append(sigmoid(np.dot(w, X_test[j]+b)))\n",
    "\n",
    "                \n",
    "        train_loss.append(logloss(y_train,y_train_pred))\n",
    "        test_loss.append(logloss(y_test,y_test_pred))\n",
    "            \n",
    "    return train_loss,test_loss,w,b                           \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
